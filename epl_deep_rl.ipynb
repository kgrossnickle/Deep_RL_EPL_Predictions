{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "epl_deep_rl.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kgrossnickle/Deep_RL_EPL_Predictions/blob/master/epl_deep_rl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PYQAfsSEZe5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow-gpu\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ic8weG5JEuv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKOAnW0bifM7",
        "colab_type": "text"
      },
      "source": [
        "#The Deep Q Learning Class\n",
        "\n",
        "Implementing Q Learning in this class and using Nueral network to make function approximation for optimal Q Value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6iTIwT3OS1T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "class deep_q_learning_agent(object):\n",
        "    def __init__(self, alpha, gamma, memory_size, num_actions, epsilon, batch_size,\n",
        "                 num_games, input_dims=(210,160,4), epsilon_decay_close_to_one=0.9,\n",
        "                 epsilon_min_val=0.0, save_dir='/content/drive/My Drive/Colab Notebooks/epl_data/saved_models/'):\n",
        "        self.action_space = [i for i in range(num_actions)]\n",
        "        self.num_games = num_games\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_decay_close_to_one = epsilon_decay_close_to_one\n",
        "        self.epsilon_min_val = epsilon_min_val\n",
        "        self.gamma = gamma\n",
        "        self.num_actions = num_actions\n",
        "        self.memory_size = memory_size\n",
        "        self.memory_loc = 0\n",
        "        self.batch_size = batch_size\n",
        "        self.q_val_for_cur_state = deep_q_learning_nn(alpha, num_actions, input_dims=input_dims,\n",
        "                                   name='q_val_for_cur_state', chkpt_dir=save_dir)\n",
        "        self.state_memory = np.zeros((self.memory_size, *input_dims))\n",
        "        self.new_state_mem = np.zeros((self.memory_size, *input_dims))\n",
        "        self.action_mem = np.zeros((self.memory_size, self.num_actions),\n",
        "                                      dtype=np.int8)\n",
        "        self.reward_mem = np.zeros(self.memory_size)\n",
        "        self.is_terminal_mem = np.zeros(self.memory_size, dtype=np.int8)\n",
        "\n",
        "    def put_transition_in_NN(self, state, action, reward, next_state, terminal):\n",
        "        idx = self.memory_loc % self.memory_size\n",
        "        self.state_memory[idx] = state\n",
        "        actions = np.zeros(self.num_actions)\n",
        "        actions[action] = 1.0\n",
        "        self.action_mem[idx] = actions\n",
        "        self.reward_mem[idx] = reward\n",
        "        self.new_state_mem[idx] = next_state\n",
        "        self.is_terminal_mem[idx] = 1 - terminal\n",
        "        self.memory_loc += 1\n",
        "\n",
        "    def take_action_based_on_epsilon(self, state):\n",
        "        cur_state = state[np.newaxis, :]\n",
        "        rand_0_to_1 = np.random.random()\n",
        "        if rand_0_to_1 < self.epsilon:\n",
        "            action_taken = np.random.choice(self.action_space)\n",
        "        else:\n",
        "            actions = self.q_val_for_cur_state.sess.run(self.q_val_for_cur_state.Q_values,\n",
        "                      feed_dict={self.q_val_for_cur_state.input: cur_state} )\n",
        "            action_taken = np.argmax(actions)\n",
        "\n",
        "        return action_taken\n",
        "\n",
        "    def save_models(self):\n",
        "        self.q_val_for_cur_state.save_model()\n",
        "\n",
        "    def load_models(self):\n",
        "        self.epsilon = 0.0 \n",
        "        self.epsilon_min_val =0.0\n",
        "        self.q_val_for_cur_state.load_model()\n",
        "    def learn_from_transition(self):\n",
        "        max_mem = None\n",
        "        if self.memory_loc < self.memory_size :\n",
        "            max_mem = self.memory_loc\n",
        "        else :\n",
        "            max_mem = self.memory_size\n",
        "\n",
        "        cur_random_batch = np.random.choice(max_mem, self.batch_size)\n",
        "        cur_reward_batch = self.reward_mem[cur_random_batch]\n",
        "        enter_new_state_batch = self.new_state_mem[cur_random_batch]\n",
        "        terminal_batch = self.is_terminal_mem[cur_random_batch]\n",
        "        cur_state_space_batch = self.state_memory[cur_random_batch]\n",
        "        cur_action_set_batch_batch = self.action_mem[cur_random_batch]\n",
        "        cur_action_vals = np.array(self.action_space, dtype=np.int8)\n",
        "        action_idxs = np.dot(cur_action_set_batch_batch, cur_action_vals)\n",
        "\n",
        "\n",
        "        q_val_for_cur_state = self.q_val_for_cur_state.sess.run(self.q_val_for_cur_state.Q_values,\n",
        "                                     feed_dict={self.q_val_for_cur_state.input: cur_state_space_batch})\n",
        "\n",
        "        q_val_for_next_state = self.q_val_for_cur_state.sess.run(self.q_val_for_cur_state.Q_values,\n",
        "                    feed_dict={self.q_val_for_cur_state.input: enter_new_state_batch})\n",
        "\n",
        "        optimal_q_from_bellman = q_val_for_cur_state.copy()\n",
        "        batch_index = np.arange(self.batch_size, dtype=np.int32)\n",
        "\n",
        "        # Fixed this line to use all actions\n",
        "        #correctly implements bellman target q val now\n",
        "        optimal_q_from_bellman[batch_index,action_idxs] = cur_reward_batch + (self.gamma*(np.max(q_val_for_next_state, axis=1)*terminal_batch))\n",
        "\n",
        "        self.q_val_for_cur_state.sess.run(self.q_val_for_cur_state.train_object,\n",
        "                        feed_dict={self.q_val_for_cur_state.input: cur_state_space_batch,\n",
        "                                   self.q_val_for_cur_state.actions: cur_action_set_batch_batch,\n",
        "                                   self.q_val_for_cur_state.optimal_q_from_bellman: optimal_q_from_bellman})\n",
        "        if self.epsilon > self.epsilon_min_val:\n",
        "            self.epsilon = self.epsilon*self.epsilon_decay_close_to_one \n",
        "        else :\n",
        "            self.epsilon = self.epsilon_min_val\n",
        "\n",
        "\n",
        "\n",
        "class deep_q_learning_nn(object):\n",
        "    def __init__(self, alpha_val, num_actions, name, input_dims,\n",
        "                 fc1_dims=256, fc2_dims=256, chkpt_dir='/content/drive/My Drive/Colab Notebooks/epl_data/saved_models/deppqinfo'):\n",
        "        self.alpha_val = alpha_val\n",
        "        self.chkpt_dir = chkpt_dir\n",
        "        self.input_dims = input_dims\n",
        "        self.num_actions = num_actions\n",
        "        self.name = name\n",
        "        self.fc1_dims = fc1_dims\n",
        "        self.fc2_dims = fc2_dims\n",
        "        self.sess = tf.Session()\n",
        "        self.build_NN()\n",
        "        self.sess.run(tf.global_variables_initializer())\n",
        "        self.saver = tf.compat.v1.train.Saver()\n",
        "        self.save_fie = os.path.join(chkpt_dir,'tf_deepqnet.ckpt')\n",
        "        self.trainable_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
        "                                        scope=self.name)\n",
        "    def build_NN(self):\n",
        "        with tf.variable_scope(self.name):\n",
        "            self.input = tf.compat.v1.placeholder (tf.float32,\n",
        "                                        shape=[None, *self.input_dims],\n",
        "                                        name='inputs')\n",
        "            self.actions = tf.compat.v1.placeholder (tf.float32,\n",
        "                                          shape=[None, self.num_actions],\n",
        "                                          name='action_taken')\n",
        "            self.optimal_q_from_bellman = tf.compat.v1.placeholder (tf.float32,\n",
        "                                           shape=[None, self.num_actions],\n",
        "                                           name='q_value')\n",
        "\n",
        "            flattened_input = tf.layers.flatten(self.input)\n",
        "            dense_layer_1 = tf.layers.dense(flattened_input, units=self.fc1_dims,\n",
        "                                     activation=tf.nn.relu,)\n",
        "            dense_layer_2 = tf.layers.dense(dense_layer_1, units=self.fc2_dims,\n",
        "                                     activation=tf.nn.relu,)\n",
        "            self.Q_values = tf.layers.dense(dense_layer_2, units=self.num_actions,)\n",
        "\n",
        "            self.loss = tf.reduce_mean(tf.square(self.Q_values - self.optimal_q_from_bellman))\n",
        "            self.train_object = tf.compat.v1.train.AdamOptimizer(self.alpha_val).minimize(self.loss)\n",
        "\n",
        "\n",
        "\n",
        "    def save_model(self):\n",
        "        self.saver.save(self.sess, self.save_fie)\n",
        "\n",
        "    def load_model(self):\n",
        "        print(self.save_fie)\n",
        "        self.saver =tf.train.import_meta_graph(self.save_fie+\".meta\")\n",
        "        self.saver.restore(self.sess, tf.train.latest_checkpoint(\"/content/drive/My Drive/Colab Notebooks/epl_data/saved_models/\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybiazX5hi4Sf",
        "colab_type": "text"
      },
      "source": [
        "**Environment Class**\n",
        "\n",
        "How we interact with the environment.\n",
        "\n",
        "We store avg values for last 15 Home games and 15 away games from team in two seperate arrays.\n",
        "\n",
        "We also implement steps here (so a step would return who actually won the game and what were the stats, and also the next teams past 15 games for the next prediction)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5c1DCgiZiEcb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class environment():\n",
        "  def __init__(self):\n",
        "    df = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/epl_data/season-1617_csv.csv\")\n",
        "    df = df.drop(columns=[\"Date\",\"Div\",\"Referee\",\"HTR\"])\n",
        "    df2 = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/epl_data/season-1718_csv.csv\")\n",
        "    df2 = df2.drop(columns=[\"Date\",\"Div\",\"Referee\",\"HTR\"])\n",
        "    df = df.append(df2, ignore_index = True, sort=False)\n",
        "    df3 = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/epl_data/season-1819_csv.csv\")\n",
        "    df3 = df3.drop(columns=[\"Date\",\"Div\",\"Referee\",\"HTR\"])\n",
        "    df = df.append(df3, ignore_index = True , sort=False)\n",
        "    #print (df)\n",
        "    self.teams_idx_val_array = []\n",
        "    for i in range(len(df)):\n",
        "      actual_outcome = df.at[i, \"FTR\"]\n",
        "      if actual_outcome == \"A\":\n",
        "        actual_outcome = np.float64(0)\n",
        "      elif actual_outcome == \"D\":\n",
        "        actual_outcome = np.float64(1)\n",
        "      else:\n",
        "        actual_outcome = np.float64(2)\n",
        "      df.at[i,\"FTR\"] = actual_outcome\n",
        "      \n",
        "      if df.at[i,\"HomeTeam\"] not in self.teams_idx_val_array:\n",
        "        self.teams_idx_val_array.append(df.at[i,\"HomeTeam\"])\n",
        "      \n",
        "      df.at[i,\"HomeTeam\"] = self.teams_idx_val_array.index(df.at[i,\"HomeTeam\"])\n",
        "      \n",
        "      if df.at[i,\"AwayTeam\"] not in self.teams_idx_val_array:\n",
        "        self.teams_idx_val_array.append(df.at[i,\"AwayTeam\"])\n",
        "      \n",
        "      df.at[i,\"AwayTeam\"] = self.teams_idx_val_array.index(df.at[i,\"AwayTeam\"])\n",
        "    \n",
        "    #print (df)\n",
        " \n",
        "    \n",
        "    \n",
        "    # NOTE, Away/Home isn't used here. Home is stats FOR team, away is stats AGAINST team\n",
        "    #self.home_team_15_game_state = pd.DataFrame(columns=['Team', 'Num_Games', 'FTHG', 'FTAG', 'FTR', 'HTHG', 'HTAG',\n",
        "    #   'HS', 'AS', 'HST', 'AST', 'HF', 'AF', 'HC', 'AC', 'HY', 'AY',\n",
        "    #   'HR', 'AR'])\n",
        "    self.home_team_15_game_state = pd.DataFrame(columns=['Team', 'Num_Games', 'FTHG', 'FTAG', 'FTR'])\n",
        "    self.away_team_15_game_state = pd.DataFrame(columns=['Team', 'Num_Games', 'FTHG', 'FTAG', 'FTR'])\n",
        "    \n",
        "    df.drop(df.iloc[:,21:(len(df.columns))], axis=1, inplace=True)\n",
        "    self.df = df\n",
        "        \n",
        "    #print (self.df.columns)  \n",
        "    self.cur_line = 0\n",
        "    self.number_of_games = len(self.df)-1\n",
        "    '''\n",
        "    make all teams num 0 - 19\n",
        "    number referees as well\n",
        "    '''\n",
        "    \n",
        "  def update_team_info(self, home_team, away_team):\n",
        "    \n",
        "    if home_team not in self.home_team_15_game_state['Team'].values:\n",
        "      dic = {'Team' : int(home_team)}\n",
        "      self.home_team_15_game_state = self.home_team_15_game_state.append(dic , ignore_index=True)\n",
        "      self.home_team_15_game_state  = self.home_team_15_game_state.fillna(0)\n",
        "      \n",
        "\n",
        "    row = self.home_team_15_game_state.loc[self.home_team_15_game_state['Team']==home_team].index[0]\n",
        "    self.home_team_15_game_state.at[row, \"Num_Games\"] = min(self.home_team_15_game_state.at[row, \"Num_Games\"] +1 , 10)\n",
        "    num_games = self.home_team_15_game_state.at[row, \"Num_Games\"]\n",
        "\n",
        "    for col in self.home_team_15_game_state.columns:\n",
        "      if str(col) != \"Num_Games\" and str(col) != 'Team':\n",
        "        self.home_team_15_game_state.at[row, col] = (self.home_team_15_game_state.at[row, col].astype(np.float64)*(num_games-1) + self.df.at[self.cur_line,col].astype(np.float64))/num_games\n",
        "\n",
        "\n",
        "    if away_team not in self.away_team_15_game_state['Team'].values:\n",
        "      dic = { 'Team' : away_team}\n",
        "      self.away_team_15_game_state = self.away_team_15_game_state.append(dic , ignore_index=True)\n",
        "      self.away_team_15_game_state = self.away_team_15_game_state.fillna(0)\n",
        "\n",
        "    row = self.away_team_15_game_state.loc[self.away_team_15_game_state['Team']==away_team].index[0]\n",
        "    self.away_team_15_game_state.at[row, \"Num_Games\"] = min(self.away_team_15_game_state.at[row, \"Num_Games\"] +1 , 10)\n",
        "    num_games = self.away_team_15_game_state.at[row, \"Num_Games\"]\n",
        "    for col in self.away_team_15_game_state.columns:\n",
        "      if str(col) != \"Num_Games\" and str(col) != \"Team\":\n",
        "        if str(col) == \"FTR\":\n",
        "          if self.away_team_15_game_state.at[row, col] == 0.0:\n",
        "            self.away_team_15_game_state.at[row, col] = 2.0\n",
        "          elif self.away_team_15_game_state.at[row, col] == 2.0:\n",
        "            self.away_team_15_game_state.at[row, col] = 0.0\n",
        "\n",
        "        # Change home/away bc we want the same correlation for away games\n",
        "        if col.count(\"H\") == 1 and \"A\" not in col:\n",
        "          col = col.replace(\"H\",\"A\")\n",
        "        elif \"A\" in col and \"H\" not in col:\n",
        "          col = col.replace(\"A\",\"H\")\n",
        "        elif col.count(\"H\") == 2:\n",
        "          col = col[0:2] + 'A' + col[2+1:]\n",
        "        else:\n",
        "          col = col.replace(\"A\",\"H\")\n",
        "        self.away_team_15_game_state.at[row, col] = (self.away_team_15_game_state.at[row, col]*(num_games-1) + self.df.at[self.cur_line,col])/num_games\n",
        "          \n",
        "    \n",
        " \n",
        "  def get_team_info(self, home_team , away_team , is_swapped):\n",
        "    ht = self.home_team_15_game_state.loc[self.home_team_15_game_state['Team'] == home_team]\n",
        "    at = self.away_team_15_game_state.loc[self.away_team_15_game_state['Team'] == away_team]\n",
        "    if ht.empty :\n",
        "      ht = ht.append({\"Team\" : home_team} , ignore_index=True)\n",
        "      ht = ht.fillna(0)\n",
        "    if at.empty :\n",
        "      at = at.append({\"Team\" : away_team} , ignore_index=True)\n",
        "      at = at.fillna(0)\n",
        "    ht = ht.drop(columns=[\"Num_Games\",\"Team\"])\n",
        "    at = at.drop(columns=[\"Num_Games\",\"Team\"])\n",
        "    ret = np.append(ht.to_numpy()[0] , at.to_numpy()[0])\n",
        "    if is_swapped:\n",
        "      temp = ret[:3].copy()\n",
        "      ret[:3] = ret[3:]\n",
        "      ret[3:] = temp\n",
        "    return ret\n",
        "\n",
        "\n",
        "  def get_reward_and_outcome(self, agent_predicted_outcome, is_swapped):\n",
        "    actual_outcome = self.df.at[self.cur_line, \"FTR\"]\n",
        "    if is_swapped and actual_outcome == 0:\n",
        "      actual_outcome = 2\n",
        "    elif is_swapped and actual_outcome == 2:\n",
        "      actual_outcome = 0\n",
        "\n",
        "\n",
        "    home_team = self.df.at[self.cur_line , \"HomeTeam\"]\n",
        "    away_team = self.df.at[self.cur_line , \"AwayTeam\"]\n",
        "    \n",
        "    self.update_team_info(home_team, away_team)\n",
        "    \n",
        "    \n",
        "\n",
        "    if agent_predicted_outcome == actual_outcome and actual_outcome == 1:\n",
        "      reward = 1\n",
        "    elif agent_predicted_outcome == actual_outcome and actual_outcome == 2:\n",
        "      reward = 1\n",
        "    elif agent_predicted_outcome == actual_outcome and actual_outcome == 0:\n",
        "      reward = 1\n",
        "    else:\n",
        "      reward = -1\n",
        "    \n",
        "    self.cur_line +=1\n",
        "    new_home_team = self.df.at[self.cur_line , \"HomeTeam\"]\n",
        "    new_away_team = self.df.at[self.cur_line , \"AwayTeam\"]\n",
        "    \n",
        "    return self.get_team_info(home_team, away_team,is_swapped) , self.get_team_info(new_home_team, new_away_team,False), reward\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGUqT3mmkC5B",
        "colab_type": "text"
      },
      "source": [
        "**Training / Running**\n",
        "\n",
        "If training, we get the array of past home/away 15 games for the home and away teams. We then make our predicition from the Q function, and learn from the actual result that happened. We also update the 15 games arrays with the current game.\n",
        "\n",
        "If we are just running the model, we do the same as above but elect not to learn form the actual result."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49-FpeW5OyFL",
        "colab_type": "code",
        "outputId": "fce18499-8014-4938-80de-3152843a094f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "import random\n",
        "import time\n",
        "tf.reset_default_graph()     \n",
        "env = environment()\n",
        "alpha = .1\n",
        "num_dims = 6\n",
        "train = False\n",
        "threshold = .58\n",
        "eps_decay = .995\n",
        "if train == True:\n",
        "  eps = 1.0\n",
        "  eps_min = .2\n",
        "else:\n",
        "  eps = 0.0\n",
        "  eps_min =0.0\n",
        "agent = deep_q_learning_agent(gamma=1, epsilon=eps, alpha=alpha, input_dims=[num_dims], num_actions=3,  \\\n",
        "                              memory_size=5000000, num_games=env.number_of_games, batch_size=512, epsilon_min_val=eps_min ,  \\\n",
        "                              epsilon_decay_close_to_one = eps_decay)\n",
        "\n",
        "\n",
        "#load Pretrained model if you want\n",
        "if train == False:\n",
        "  agent.load_models()\n",
        "\n",
        "list_of_scores = []\n",
        "list_of_test_scores = []\n",
        "epsilons_values_over_time = []\n",
        "score = 0\n",
        "\n",
        "# 0s array\n",
        "cur_team_info = np.zeros( num_dims ) #env.step(0)\n",
        "league_table = [0] * len( env.teams_idx_val_array ) \n",
        "i=0\n",
        "backthrough=0\n",
        "is_swapped = False\n",
        "\n",
        "while (True):\n",
        "\n",
        "   \n",
        "\n",
        "    \n",
        "    score = 0\n",
        "    have_finished_episode = False\n",
        "    #print (\"SHOULD BE SAME AS ABOVE : \" + str(cur_team_info[0]))\n",
        "    cur_action = agent.take_action_based_on_epsilon(cur_team_info)\n",
        "    if i %10 ==0:\n",
        "      #print(cur_action)\n",
        "      #print('episode: ', i,'score: ', score)\n",
        "      average_score = np.mean(list_of_scores[max(0, i-100):(i)])\n",
        "      #print('100 averages -- episode: ', i,'score: ', list_of_scores[-1:],\n",
        "      #        ' mean score %.2f' % average_score,\n",
        "      #        'epsilon_val %.2f' % agent.epsilon)\n",
        "   \n",
        "\n",
        "    actual_outcome ,new_team_info, reward = env.get_reward_and_outcome(cur_action,False)\n",
        "    # if random.random() > .5 : \n",
        "    #   temp = new_team_info[:3].copy()\n",
        "    #   new_team_info[:3] = new_team_info[3:]\n",
        "    #   new_team_info[3:] = temp\n",
        "    #   is_swapped = True\n",
        "    # else:\n",
        "    #   is_swapped = False\n",
        "    if i > 760:\n",
        "      actual_res = env.df.at[env.cur_line-1, \"FTR\"]\n",
        "      actual_res = cur_action\n",
        "      if actual_res == 0:\n",
        "        league_table[int(cur_team_info[5])] +=3\n",
        "      elif actual_res == 1:\n",
        "        league_table[int(cur_team_info[0])] +=1\n",
        "        league_table[int(cur_team_info[5])] +=1\n",
        "      else:\n",
        "        league_table[int(cur_team_info[0])] +=3\n",
        "    score += reward\n",
        "    if train == True:\n",
        "      agent.put_transition_in_NN(cur_team_info, cur_action,\n",
        "                            reward, actual_outcome, int(1))\n",
        "      agent.learn_from_transition()\n",
        "    cur_team_info = new_team_info\n",
        "\n",
        "    epsilons_values_over_time.append(agent.epsilon)\n",
        "    if score > 1.0:\n",
        "      score = 1.0\n",
        "    elif score < 0:\n",
        "      score = 0\n",
        "    list_of_scores.append(score)\n",
        "    if i >= 760:\n",
        "      if backthrough <1:\n",
        "        i=0\n",
        "        env.cur_line = 0\n",
        "        backthrough += 1\n",
        "      else:\n",
        "        agent.epsilon_min_val = 0.0\n",
        "        agent.epsilon =0.0\n",
        "      list_of_test_scores.append(score)\n",
        "      #if i % 10 ==0 :\n",
        "        #print(\"Of \"+str(len(list_of_test_scores))+\" Games in 18-19 EPL, we predicted \"+ str(np.mean(list_of_test_scores) ))\n",
        "    i+=1\n",
        "    if i >= len(env.df):\n",
        "      break\n",
        "# for i in range (len(env.teams_idx_val_array)):\n",
        "#   if league_table[i] >0 :\n",
        "#     print (env.teams_idx_val_array[i]+ \" \"+str(league_table[i])+\" pts\")\n",
        "print(\"\\n\\n Of \"+str(len(list_of_test_scores))+\" Games in 18-19 EPL, we predicted \"+ str(np.mean(list_of_test_scores))[:4] +\" Correctly\" )\n",
        "if train == True and np.mean(list_of_test_scores) > threshold :\n",
        "  agent.save_models()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/epl_data/saved_models/tf_deepqnet.ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
            "  out=out, **kwargs)\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            " Of 381 Games in 18-19 EPL, we predicted 0.58 Correctly\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}