{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "epl_deep_rl.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kgrossnickle/Deep_RL_EPL_Predictions/blob/master/epl_deep_rl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PYQAfsSEZe5",
        "colab_type": "code",
        "outputId": "5b7ab9ac-1ec8-4aae-812b-d38c5fd01562",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        }
      },
      "source": [
        "!pip install tensorflow-gpu\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/04/43153bfdfcf6c9a4c38ecdb971ca9a75b9a791bb69a764d652c359aca504/tensorflow_gpu-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (377.0MB)\n",
            "\u001b[K     |████████████████████████████████| 377.0MB 76kB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.8.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.11.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.12.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.7.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.0.8)\n",
            "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.14.0)\n",
            "Requirement already satisfied: tensorboard<1.15.0,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.14.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.16.4)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.7.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.1.7)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.33.6)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.2.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.15.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu) (2.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu) (3.1.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu) (41.2.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu) (0.15.5)\n",
            "Installing collected packages: tensorflow-gpu\n",
            "Successfully installed tensorflow-gpu-1.14.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ic8weG5JEuv",
        "colab_type": "code",
        "outputId": "7ff6d849-c835-40b6-957d-3f477fc14f44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKOAnW0bifM7",
        "colab_type": "text"
      },
      "source": [
        "The Deep Q Learning Class\n",
        "\n",
        "Implementing Q Learning in this class and using Nueral network to make function approximation for optimal Q Value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6iTIwT3OS1T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "class deep_q_learning_agent(object):\n",
        "    def __init__(self, alpha, gamma, memory_size, num_actions, epsilon, batch_size,\n",
        "                 num_games, input_dims=(210,160,4), epsilon_decay_close_to_one=0.9,\n",
        "                 epsilon_min_val=0.0, save_dir='/content/drive/My Drive/Colab Notebooks/epl_data/saved_models/'):\n",
        "        self.action_space = [i for i in range(num_actions)]\n",
        "        self.num_games = num_games\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_decay_close_to_one = epsilon_decay_close_to_one\n",
        "        self.epsilon_min_val = epsilon_min_val\n",
        "        self.gamma = gamma\n",
        "        self.num_actions = num_actions\n",
        "        self.memory_size = memory_size\n",
        "        self.memory_loc = 0\n",
        "        self.batch_size = batch_size\n",
        "        self.q_val_for_cur_state = deep_q_learning_nn(alpha, num_actions, input_dims=input_dims,\n",
        "                                   name='q_val_for_cur_state', chkpt_dir=save_dir)\n",
        "        self.state_memory = np.zeros((self.memory_size, *input_dims))\n",
        "        self.new_state_mem = np.zeros((self.memory_size, *input_dims))\n",
        "        self.action_mem = np.zeros((self.memory_size, self.num_actions),\n",
        "                                      dtype=np.int8)\n",
        "        self.reward_mem = np.zeros(self.memory_size)\n",
        "        self.is_terminal_mem = np.zeros(self.memory_size, dtype=np.int8)\n",
        "\n",
        "    def put_transition_in_NN(self, state, action, reward, next_state, terminal):\n",
        "        idx = self.memory_loc % self.memory_size\n",
        "        self.state_memory[idx] = state\n",
        "        actions = np.zeros(self.num_actions)\n",
        "        actions[action] = 1.0\n",
        "        self.action_mem[idx] = actions\n",
        "        self.reward_mem[idx] = reward\n",
        "        self.new_state_mem[idx] = next_state\n",
        "        self.is_terminal_mem[idx] = 1 - terminal\n",
        "        self.memory_loc += 1\n",
        "\n",
        "    def take_action_based_on_epsilon(self, state):\n",
        "        cur_state = state[np.newaxis, :]\n",
        "        rand_0_to_1 = np.random.random()\n",
        "        if rand_0_to_1 < self.epsilon:\n",
        "            action_taken = np.random.choice(self.action_space)\n",
        "        else:\n",
        "            actions = self.q_val_for_cur_state.sess.run(self.q_val_for_cur_state.Q_values,\n",
        "                      feed_dict={self.q_val_for_cur_state.input: cur_state} )\n",
        "            action_taken = np.argmax(actions)\n",
        "\n",
        "        return action_taken\n",
        "\n",
        "    def save_models(self):\n",
        "        self.q_val_for_cur_state.save_model()\n",
        "\n",
        "    def load_models(self):\n",
        "        self.epsilon = 0.0 \n",
        "        self.epsilon_min_val =0.0\n",
        "        self.q_val_for_cur_state.load_model()\n",
        "    def learn_from_transition(self):\n",
        "        max_mem = None\n",
        "        if self.memory_loc < self.memory_size :\n",
        "            max_mem = self.memory_loc\n",
        "        else :\n",
        "            max_mem = self.memory_size\n",
        "\n",
        "        cur_random_batch = np.random.choice(max_mem, self.batch_size)\n",
        "        cur_reward_batch = self.reward_mem[cur_random_batch]\n",
        "        enter_new_state_batch = self.new_state_mem[cur_random_batch]\n",
        "        terminal_batch = self.is_terminal_mem[cur_random_batch]\n",
        "        cur_state_space_batch = self.state_memory[cur_random_batch]\n",
        "        cur_action_set_batch_batch = self.action_mem[cur_random_batch]\n",
        "        cur_action_vals = np.array(self.action_space, dtype=np.int8)\n",
        "        action_idxs = np.dot(cur_action_set_batch_batch, cur_action_vals)\n",
        "\n",
        "\n",
        "        q_val_for_cur_state = self.q_val_for_cur_state.sess.run(self.q_val_for_cur_state.Q_values,\n",
        "                                     feed_dict={self.q_val_for_cur_state.input: cur_state_space_batch})\n",
        "\n",
        "        q_val_for_next_state = self.q_val_for_cur_state.sess.run(self.q_val_for_cur_state.Q_values,\n",
        "                    feed_dict={self.q_val_for_cur_state.input: enter_new_state_batch})\n",
        "\n",
        "        optimal_q_from_bellman = q_val_for_cur_state.copy()\n",
        "        batch_index = np.arange(self.batch_size, dtype=np.int32)\n",
        "\n",
        "        # Fixed this line to use all actions\n",
        "        #correctly implements bellman target q val now\n",
        "        optimal_q_from_bellman[batch_index,action_idxs] = cur_reward_batch + (self.gamma*(np.max(q_val_for_next_state, axis=1)*terminal_batch))\n",
        "\n",
        "        self.q_val_for_cur_state.sess.run(self.q_val_for_cur_state.train_object,\n",
        "                        feed_dict={self.q_val_for_cur_state.input: cur_state_space_batch,\n",
        "                                   self.q_val_for_cur_state.actions: cur_action_set_batch_batch,\n",
        "                                   self.q_val_for_cur_state.optimal_q_from_bellman: optimal_q_from_bellman})\n",
        "        if self.epsilon > self.epsilon_min_val:\n",
        "            self.epsilon = self.epsilon*self.epsilon_decay_close_to_one \n",
        "        else :\n",
        "            self.epsilon = self.epsilon_min_val\n",
        "\n",
        "\n",
        "\n",
        "class deep_q_learning_nn(object):\n",
        "    def __init__(self, alpha_val, num_actions, name, input_dims,\n",
        "                 fc1_dims=256, fc2_dims=256, chkpt_dir='/content/drive/My Drive/Colab Notebooks/epl_data/saved_models/deppqinfo'):\n",
        "        self.alpha_val = alpha_val\n",
        "        self.chkpt_dir = chkpt_dir\n",
        "        self.input_dims = input_dims\n",
        "        self.num_actions = num_actions\n",
        "        self.name = name\n",
        "        self.fc1_dims = fc1_dims\n",
        "        self.fc2_dims = fc2_dims\n",
        "        self.sess = tf.Session()\n",
        "        self.build_NN()\n",
        "        self.sess.run(tf.global_variables_initializer())\n",
        "        self.saver = tf.compat.v1.train.Saver()\n",
        "        self.save_fie = os.path.join(chkpt_dir,'tf_deepqnet.ckpt')\n",
        "        self.trainable_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
        "                                        scope=self.name)\n",
        "    def build_NN(self):\n",
        "        with tf.variable_scope(self.name):\n",
        "            self.input = tf.compat.v1.placeholder (tf.float32,\n",
        "                                        shape=[None, *self.input_dims],\n",
        "                                        name='inputs')\n",
        "            self.actions = tf.compat.v1.placeholder (tf.float32,\n",
        "                                          shape=[None, self.num_actions],\n",
        "                                          name='action_taken')\n",
        "            self.optimal_q_from_bellman = tf.compat.v1.placeholder (tf.float32,\n",
        "                                           shape=[None, self.num_actions],\n",
        "                                           name='q_value')\n",
        "\n",
        "            flattened_input = tf.layers.flatten(self.input)\n",
        "            dense_layer_1 = tf.layers.dense(flattened_input, units=self.fc1_dims,\n",
        "                                     activation=tf.nn.relu,)\n",
        "            dense_layer_2 = tf.layers.dense(dense_layer_1, units=self.fc2_dims,\n",
        "                                     activation=tf.nn.relu,)\n",
        "            self.Q_values = tf.layers.dense(dense_layer_2, units=self.num_actions,)\n",
        "\n",
        "            self.loss = tf.reduce_mean(tf.square(self.Q_values - self.optimal_q_from_bellman))\n",
        "            self.train_object = tf.compat.v1.train.AdamOptimizer(self.alpha_val).minimize(self.loss)\n",
        "\n",
        "\n",
        "\n",
        "    def save_model(self):\n",
        "        self.saver.save(self.sess, self.save_fie)\n",
        "\n",
        "    def load_model(self):\n",
        "        print(self.save_fie)\n",
        "        self.saver =tf.train.import_meta_graph(self.save_fie+\".meta\")\n",
        "        self.saver.restore(self.sess, tf.train.latest_checkpoint(\"/content/drive/My Drive/Colab Notebooks/epl_data/saved_models/\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybiazX5hi4Sf",
        "colab_type": "text"
      },
      "source": [
        "**Environment Class**\n",
        "\n",
        "How we interact with the environment.\n",
        "\n",
        "We store avg values for last 15 Home games and 15 away games from team in two seperate arrays.\n",
        "\n",
        "We also implement steps here (so a step would return who actually won the game and what were the stats, and also the next teams past 15 games for the next prediction)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5c1DCgiZiEcb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class environment():\n",
        "  def __init__(self):\n",
        "    df = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/epl_data/season-1617_csv.csv\")\n",
        "    df = df.drop(columns=[\"Date\",\"Div\",\"Referee\",\"HTR\"])\n",
        "    df2 = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/epl_data/season-1718_csv.csv\")\n",
        "    df2 = df2.drop(columns=[\"Date\",\"Div\",\"Referee\",\"HTR\"])\n",
        "    df = df.append(df2, ignore_index = True, sort=False)\n",
        "    df3 = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/epl_data/season-1819_csv.csv\")\n",
        "    df3 = df3.drop(columns=[\"Date\",\"Div\",\"Referee\",\"HTR\"])\n",
        "    df = df.append(df3, ignore_index = True , sort=False)\n",
        "    #print (df)\n",
        "    self.teams_idx_val_array = []\n",
        "    for i in range(len(df)):\n",
        "      actual_outcome = df.at[i, \"FTR\"]\n",
        "      if actual_outcome == \"A\":\n",
        "        actual_outcome = np.float64(0)\n",
        "      elif actual_outcome == \"D\":\n",
        "        actual_outcome = np.float64(1)\n",
        "      else:\n",
        "        actual_outcome = np.float64(2)\n",
        "      df.at[i,\"FTR\"] = actual_outcome\n",
        "      \n",
        "      if df.at[i,\"HomeTeam\"] not in self.teams_idx_val_array:\n",
        "        self.teams_idx_val_array.append(df.at[i,\"HomeTeam\"])\n",
        "      \n",
        "      df.at[i,\"HomeTeam\"] = self.teams_idx_val_array.index(df.at[i,\"HomeTeam\"])\n",
        "      \n",
        "      if df.at[i,\"AwayTeam\"] not in self.teams_idx_val_array:\n",
        "        self.teams_idx_val_array.append(df.at[i,\"AwayTeam\"])\n",
        "      \n",
        "      df.at[i,\"AwayTeam\"] = self.teams_idx_val_array.index(df.at[i,\"AwayTeam\"])\n",
        "    \n",
        "    #print (df)\n",
        " \n",
        "    \n",
        "    \n",
        "    # NOTE, Away/Home isn't used here. Home is stats FOR team, away is stats AGAINST team\n",
        "    #self.home_team_15_game_state = pd.DataFrame(columns=['Team', 'Num_Games', 'FTHG', 'FTAG', 'FTR', 'HTHG', 'HTAG',\n",
        "    #   'HS', 'AS', 'HST', 'AST', 'HF', 'AF', 'HC', 'AC', 'HY', 'AY',\n",
        "    #   'HR', 'AR'])\n",
        "    self.home_team_15_game_state = pd.DataFrame(columns=['Team', 'Num_Games', 'FTHG', 'FTAG', 'FTR'])\n",
        "    self.away_team_15_game_state = pd.DataFrame(columns=['Team', 'Num_Games', 'FTHG', 'FTAG', 'FTR'])\n",
        "    \n",
        "    df.drop(df.iloc[:,21:(len(df.columns))], axis=1, inplace=True)\n",
        "    self.df = df\n",
        "        \n",
        "    #print (self.df.columns)  \n",
        "    self.cur_line = 0\n",
        "    self.number_of_games = len(self.df)-1\n",
        "    '''\n",
        "    make all teams num 0 - 19\n",
        "    number referees as well\n",
        "    '''\n",
        "    \n",
        "  def update_team_info(self, home_team, away_team):\n",
        "    \n",
        "    if home_team not in self.home_team_15_game_state['Team'].values:\n",
        "      dic = {'Team' : int(home_team)}\n",
        "      self.home_team_15_game_state = self.home_team_15_game_state.append(dic , ignore_index=True)\n",
        "      self.home_team_15_game_state  = self.home_team_15_game_state.fillna(0)\n",
        "      \n",
        "\n",
        "    row = self.home_team_15_game_state.loc[self.home_team_15_game_state['Team']==home_team].index[0]\n",
        "    self.home_team_15_game_state.at[row, \"Num_Games\"] = min(self.home_team_15_game_state.at[row, \"Num_Games\"] +1 , 10)\n",
        "    num_games = self.home_team_15_game_state.at[row, \"Num_Games\"]\n",
        "\n",
        "    for col in self.home_team_15_game_state.columns:\n",
        "      if str(col) != \"Num_Games\" and str(col) != 'Team':\n",
        "        self.home_team_15_game_state.at[row, col] = (self.home_team_15_game_state.at[row, col].astype(np.float64)*(num_games-1) + self.df.at[self.cur_line,col].astype(np.float64))/num_games\n",
        "\n",
        "\n",
        "    if away_team not in self.away_team_15_game_state['Team'].values:\n",
        "      dic = { 'Team' : away_team}\n",
        "      self.away_team_15_game_state = self.away_team_15_game_state.append(dic , ignore_index=True)\n",
        "      self.away_team_15_game_state = self.away_team_15_game_state.fillna(0)\n",
        "\n",
        "    row = self.away_team_15_game_state.loc[self.away_team_15_game_state['Team']==away_team].index[0]\n",
        "    self.away_team_15_game_state.at[row, \"Num_Games\"] = min(self.away_team_15_game_state.at[row, \"Num_Games\"] +1 , 10)\n",
        "    num_games = self.away_team_15_game_state.at[row, \"Num_Games\"]\n",
        "    for col in self.away_team_15_game_state.columns:\n",
        "      if str(col) != \"Num_Games\" and str(col) != \"Team\":\n",
        "        if str(col) == \"FTR\":\n",
        "          if self.away_team_15_game_state.at[row, col] == 0.0:\n",
        "            self.away_team_15_game_state.at[row, col] = 2.0\n",
        "          elif self.away_team_15_game_state.at[row, col] == 2.0:\n",
        "            self.away_team_15_game_state.at[row, col] = 0.0\n",
        "\n",
        "        # Change home/away bc we want the same correlation for away games\n",
        "        if col.count(\"H\") == 1 and \"A\" not in col:\n",
        "          col = col.replace(\"H\",\"A\")\n",
        "        elif \"A\" in col and \"H\" not in col:\n",
        "          col = col.replace(\"A\",\"H\")\n",
        "        elif col.count(\"H\") == 2:\n",
        "          col = col[0:2] + 'A' + col[2+1:]\n",
        "        else:\n",
        "          col = col.replace(\"A\",\"H\")\n",
        "        self.away_team_15_game_state.at[row, col] = (self.away_team_15_game_state.at[row, col]*(num_games-1) + self.df.at[self.cur_line,col])/num_games\n",
        "          \n",
        "    \n",
        " \n",
        "  def get_team_info(self, home_team , away_team , is_swapped):\n",
        "    ht = self.home_team_15_game_state.loc[self.home_team_15_game_state['Team'] == home_team]\n",
        "    at = self.away_team_15_game_state.loc[self.away_team_15_game_state['Team'] == away_team]\n",
        "    if ht.empty :\n",
        "      ht = ht.append({\"Team\" : home_team} , ignore_index=True)\n",
        "      ht = ht.fillna(0)\n",
        "    if at.empty :\n",
        "      at = at.append({\"Team\" : away_team} , ignore_index=True)\n",
        "      at = at.fillna(0)\n",
        "    ht = ht.drop(columns=[\"Num_Games\",\"Team\"])\n",
        "    at = at.drop(columns=[\"Num_Games\",\"Team\"])\n",
        "    ret = np.append(ht.to_numpy()[0] , at.to_numpy()[0])\n",
        "    if is_swapped:\n",
        "      temp = ret[:3].copy()\n",
        "      ret[:3] = ret[3:]\n",
        "      ret[3:] = temp\n",
        "    return ret\n",
        "\n",
        "\n",
        "  def get_reward_and_outcome(self, agent_predicted_outcome, is_swapped):\n",
        "    actual_outcome = self.df.at[self.cur_line, \"FTR\"]\n",
        "    if is_swapped and actual_outcome == 0:\n",
        "      actual_outcome = 2\n",
        "    elif is_swapped and actual_outcome == 2:\n",
        "      actual_outcome = 0\n",
        "\n",
        "\n",
        "    home_team = self.df.at[self.cur_line , \"HomeTeam\"]\n",
        "    away_team = self.df.at[self.cur_line , \"AwayTeam\"]\n",
        "    \n",
        "    self.update_team_info(home_team, away_team)\n",
        "    \n",
        "    \n",
        "\n",
        "    if agent_predicted_outcome == actual_outcome and actual_outcome == 1:\n",
        "      reward = 1\n",
        "    elif agent_predicted_outcome == actual_outcome and actual_outcome == 2:\n",
        "      reward = 1\n",
        "    elif agent_predicted_outcome == actual_outcome and actual_outcome == 0:\n",
        "      reward = 1\n",
        "    else:\n",
        "      reward = -1\n",
        "    \n",
        "    self.cur_line +=1\n",
        "    new_home_team = self.df.at[self.cur_line , \"HomeTeam\"]\n",
        "    new_away_team = self.df.at[self.cur_line , \"AwayTeam\"]\n",
        "    \n",
        "    return self.get_team_info(home_team, away_team,is_swapped) , self.get_team_info(new_home_team, new_away_team,False), reward\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGUqT3mmkC5B",
        "colab_type": "text"
      },
      "source": [
        "**Training / Running**\n",
        "\n",
        "If training, we get the array of past home/away 15 games for the home and away teams. We then make our predicition from the Q function, and learn from the actual result that happened. We also update the 15 games arrays with the current game.\n",
        "\n",
        "If we are just running the model, we do the same as above but elect not to learn form the actual result."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49-FpeW5OyFL",
        "colab_type": "code",
        "outputId": "0e41c6be-ca83-4d12-8c3a-877cd4b4e07f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import random\n",
        "import time\n",
        "tf.reset_default_graph()     \n",
        "env = environment()\n",
        "alpha = .1\n",
        "num_dims = 6\n",
        "train = False\n",
        "threshold = .58\n",
        "eps_decay = .995\n",
        "if train == True:\n",
        "  eps = 1.0\n",
        "  eps_min = .2\n",
        "else:\n",
        "  eps = 0.0\n",
        "  eps_min =0.0\n",
        "agent = deep_q_learning_agent(gamma=1, epsilon=eps, alpha=alpha, input_dims=[num_dims], num_actions=3,  \\\n",
        "                              memory_size=5000000, num_games=env.number_of_games, batch_size=512, epsilon_min_val=eps_min ,  \\\n",
        "                              epsilon_decay_close_to_one = eps_decay)\n",
        "\n",
        "\n",
        "#load Pretrained model if you want\n",
        "if train == False:\n",
        "  agent.load_models()\n",
        "\n",
        "list_of_scores = []\n",
        "list_of_test_scores = []\n",
        "epsilons_values_over_time = []\n",
        "score = 0\n",
        "\n",
        "# 0s array\n",
        "cur_team_info = np.zeros( num_dims ) #env.step(0)\n",
        "league_table = [0] * len( env.teams_idx_val_array ) \n",
        "i=0\n",
        "backthrough=0\n",
        "is_swapped = False\n",
        "\n",
        "while (True):\n",
        "\n",
        "   \n",
        "\n",
        "    \n",
        "    score = 0\n",
        "    have_finished_episode = False\n",
        "    #print (\"SHOULD BE SAME AS ABOVE : \" + str(cur_team_info[0]))\n",
        "    cur_action = agent.take_action_based_on_epsilon(cur_team_info)\n",
        "    if i %10 ==0:\n",
        "      print(cur_action)\n",
        "      print('episode: ', i,'score: ', score)\n",
        "      average_score = np.mean(list_of_scores[max(0, i-100):(i)])\n",
        "      print('100 averages -- episode: ', i,'score: ', list_of_scores[-1:],\n",
        "              ' mean score %.2f' % average_score,\n",
        "              'epsilon_val %.2f' % agent.epsilon)\n",
        "   \n",
        "\n",
        "    actual_outcome ,new_team_info, reward = env.get_reward_and_outcome(cur_action,False)\n",
        "    # if random.random() > .5 : \n",
        "    #   temp = new_team_info[:3].copy()\n",
        "    #   new_team_info[:3] = new_team_info[3:]\n",
        "    #   new_team_info[3:] = temp\n",
        "    #   is_swapped = True\n",
        "    # else:\n",
        "    #   is_swapped = False\n",
        "    if i > 760:\n",
        "      actual_res = env.df.at[env.cur_line-1, \"FTR\"]\n",
        "      actual_res = cur_action\n",
        "      if actual_res == 0:\n",
        "        league_table[int(cur_team_info[5])] +=3\n",
        "      elif actual_res == 1:\n",
        "        league_table[int(cur_team_info[0])] +=1\n",
        "        league_table[int(cur_team_info[5])] +=1\n",
        "      else:\n",
        "        league_table[int(cur_team_info[0])] +=3\n",
        "    score += reward\n",
        "    if train == True:\n",
        "      agent.put_transition_in_NN(cur_team_info, cur_action,\n",
        "                            reward, actual_outcome, int(1))\n",
        "      agent.learn_from_transition()\n",
        "    cur_team_info = new_team_info\n",
        "\n",
        "    epsilons_values_over_time.append(agent.epsilon)\n",
        "    if score > 1.0:\n",
        "      score = 1.0\n",
        "    elif score < 0:\n",
        "      score = 0\n",
        "    list_of_scores.append(score)\n",
        "    if i >= 760:\n",
        "      if backthrough <1:\n",
        "        i=0\n",
        "        env.cur_line = 0\n",
        "        backthrough += 1\n",
        "      else:\n",
        "        agent.epsilon_min_val = 0.0\n",
        "        agent.epsilon =0.0\n",
        "      list_of_test_scores.append(score)\n",
        "      if i % 10 ==0 :\n",
        "        print(\"Of \"+str(len(list_of_test_scores))+\" Games in 18-19 EPL, we predicted \"+ str(np.mean(list_of_test_scores) ))\n",
        "    i+=1\n",
        "    if i >= len(env.df):\n",
        "      break\n",
        "# for i in range (len(env.teams_idx_val_array)):\n",
        "#   if league_table[i] >0 :\n",
        "#     print (env.teams_idx_val_array[i]+ \" \"+str(league_table[i])+\" pts\")\n",
        "print(np.mean(list_of_test_scores))\n",
        "if train == True and np.mean(list_of_test_scores) > threshold :\n",
        "  agent.save_models()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/epl_data/saved_models/tf_deepqnet.ckpt\n",
            "2\n",
            "episode:  0 score:  0\n",
            "100 averages -- episode:  0 score:  []  mean score nan epsilon_val 0.00\n",
            "2\n",
            "episode:  10 score:  0\n",
            "100 averages -- episode:  10 score:  [1]  mean score 0.30 epsilon_val 0.00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
            "  out=out, **kwargs)\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2\n",
            "episode:  20 score:  0\n",
            "100 averages -- episode:  20 score:  [1]  mean score 0.35 epsilon_val 0.00\n",
            "2\n",
            "episode:  30 score:  0\n",
            "100 averages -- episode:  30 score:  [0]  mean score 0.40 epsilon_val 0.00\n",
            "2\n",
            "episode:  40 score:  0\n",
            "100 averages -- episode:  40 score:  [1]  mean score 0.38 epsilon_val 0.00\n",
            "0\n",
            "episode:  50 score:  0\n",
            "100 averages -- episode:  50 score:  [0]  mean score 0.46 epsilon_val 0.00\n",
            "0\n",
            "episode:  60 score:  0\n",
            "100 averages -- episode:  60 score:  [0]  mean score 0.47 epsilon_val 0.00\n",
            "2\n",
            "episode:  70 score:  0\n",
            "100 averages -- episode:  70 score:  [0]  mean score 0.43 epsilon_val 0.00\n",
            "2\n",
            "episode:  80 score:  0\n",
            "100 averages -- episode:  80 score:  [0]  mean score 0.45 epsilon_val 0.00\n",
            "0\n",
            "episode:  90 score:  0\n",
            "100 averages -- episode:  90 score:  [0]  mean score 0.43 epsilon_val 0.00\n",
            "2\n",
            "episode:  100 score:  0\n",
            "100 averages -- episode:  100 score:  [1]  mean score 0.47 epsilon_val 0.00\n",
            "0\n",
            "episode:  110 score:  0\n",
            "100 averages -- episode:  110 score:  [0]  mean score 0.48 epsilon_val 0.00\n",
            "0\n",
            "episode:  120 score:  0\n",
            "100 averages -- episode:  120 score:  [1]  mean score 0.50 epsilon_val 0.00\n",
            "0\n",
            "episode:  130 score:  0\n",
            "100 averages -- episode:  130 score:  [0]  mean score 0.50 epsilon_val 0.00\n",
            "2\n",
            "episode:  140 score:  0\n",
            "100 averages -- episode:  140 score:  [1]  mean score 0.53 epsilon_val 0.00\n",
            "2\n",
            "episode:  150 score:  0\n",
            "100 averages -- episode:  150 score:  [1]  mean score 0.51 epsilon_val 0.00\n",
            "0\n",
            "episode:  160 score:  0\n",
            "100 averages -- episode:  160 score:  [1]  mean score 0.54 epsilon_val 0.00\n",
            "2\n",
            "episode:  170 score:  0\n",
            "100 averages -- episode:  170 score:  [1]  mean score 0.58 epsilon_val 0.00\n",
            "2\n",
            "episode:  180 score:  0\n",
            "100 averages -- episode:  180 score:  [0]  mean score 0.59 epsilon_val 0.00\n",
            "2\n",
            "episode:  190 score:  0\n",
            "100 averages -- episode:  190 score:  [1]  mean score 0.63 epsilon_val 0.00\n",
            "2\n",
            "episode:  200 score:  0\n",
            "100 averages -- episode:  200 score:  [1]  mean score 0.61 epsilon_val 0.00\n",
            "2\n",
            "episode:  210 score:  0\n",
            "100 averages -- episode:  210 score:  [0]  mean score 0.62 epsilon_val 0.00\n",
            "2\n",
            "episode:  220 score:  0\n",
            "100 averages -- episode:  220 score:  [1]  mean score 0.62 epsilon_val 0.00\n",
            "2\n",
            "episode:  230 score:  0\n",
            "100 averages -- episode:  230 score:  [1]  mean score 0.60 epsilon_val 0.00\n",
            "2\n",
            "episode:  240 score:  0\n",
            "100 averages -- episode:  240 score:  [1]  mean score 0.62 epsilon_val 0.00\n",
            "2\n",
            "episode:  250 score:  0\n",
            "100 averages -- episode:  250 score:  [1]  mean score 0.61 epsilon_val 0.00\n",
            "2\n",
            "episode:  260 score:  0\n",
            "100 averages -- episode:  260 score:  [0]  mean score 0.59 epsilon_val 0.00\n",
            "2\n",
            "episode:  270 score:  0\n",
            "100 averages -- episode:  270 score:  [0]  mean score 0.58 epsilon_val 0.00\n",
            "2\n",
            "episode:  280 score:  0\n",
            "100 averages -- episode:  280 score:  [0]  mean score 0.58 epsilon_val 0.00\n",
            "2\n",
            "episode:  290 score:  0\n",
            "100 averages -- episode:  290 score:  [0]  mean score 0.56 epsilon_val 0.00\n",
            "2\n",
            "episode:  300 score:  0\n",
            "100 averages -- episode:  300 score:  [1]  mean score 0.57 epsilon_val 0.00\n",
            "2\n",
            "episode:  310 score:  0\n",
            "100 averages -- episode:  310 score:  [1]  mean score 0.58 epsilon_val 0.00\n",
            "0\n",
            "episode:  320 score:  0\n",
            "100 averages -- episode:  320 score:  [1]  mean score 0.59 epsilon_val 0.00\n",
            "2\n",
            "episode:  330 score:  0\n",
            "100 averages -- episode:  330 score:  [1]  mean score 0.63 epsilon_val 0.00\n",
            "2\n",
            "episode:  340 score:  0\n",
            "100 averages -- episode:  340 score:  [0]  mean score 0.58 epsilon_val 0.00\n",
            "2\n",
            "episode:  350 score:  0\n",
            "100 averages -- episode:  350 score:  [1]  mean score 0.57 epsilon_val 0.00\n",
            "0\n",
            "episode:  360 score:  0\n",
            "100 averages -- episode:  360 score:  [1]  mean score 0.59 epsilon_val 0.00\n",
            "2\n",
            "episode:  370 score:  0\n",
            "100 averages -- episode:  370 score:  [1]  mean score 0.62 epsilon_val 0.00\n",
            "2\n",
            "episode:  380 score:  0\n",
            "100 averages -- episode:  380 score:  [1]  mean score 0.62 epsilon_val 0.00\n",
            "2\n",
            "episode:  390 score:  0\n",
            "100 averages -- episode:  390 score:  [1]  mean score 0.63 epsilon_val 0.00\n",
            "0\n",
            "episode:  400 score:  0\n",
            "100 averages -- episode:  400 score:  [0]  mean score 0.61 epsilon_val 0.00\n",
            "2\n",
            "episode:  410 score:  0\n",
            "100 averages -- episode:  410 score:  [0]  mean score 0.59 epsilon_val 0.00\n",
            "2\n",
            "episode:  420 score:  0\n",
            "100 averages -- episode:  420 score:  [0]  mean score 0.57 epsilon_val 0.00\n",
            "2\n",
            "episode:  430 score:  0\n",
            "100 averages -- episode:  430 score:  [1]  mean score 0.55 epsilon_val 0.00\n",
            "2\n",
            "episode:  440 score:  0\n",
            "100 averages -- episode:  440 score:  [1]  mean score 0.60 epsilon_val 0.00\n",
            "2\n",
            "episode:  450 score:  0\n",
            "100 averages -- episode:  450 score:  [0]  mean score 0.61 epsilon_val 0.00\n",
            "2\n",
            "episode:  460 score:  0\n",
            "100 averages -- episode:  460 score:  [0]  mean score 0.55 epsilon_val 0.00\n",
            "2\n",
            "episode:  470 score:  0\n",
            "100 averages -- episode:  470 score:  [1]  mean score 0.52 epsilon_val 0.00\n",
            "2\n",
            "episode:  480 score:  0\n",
            "100 averages -- episode:  480 score:  [1]  mean score 0.51 epsilon_val 0.00\n",
            "0\n",
            "episode:  490 score:  0\n",
            "100 averages -- episode:  490 score:  [1]  mean score 0.51 epsilon_val 0.00\n",
            "2\n",
            "episode:  500 score:  0\n",
            "100 averages -- episode:  500 score:  [0]  mean score 0.53 epsilon_val 0.00\n",
            "2\n",
            "episode:  510 score:  0\n",
            "100 averages -- episode:  510 score:  [1]  mean score 0.54 epsilon_val 0.00\n",
            "2\n",
            "episode:  520 score:  0\n",
            "100 averages -- episode:  520 score:  [1]  mean score 0.54 epsilon_val 0.00\n",
            "0\n",
            "episode:  530 score:  0\n",
            "100 averages -- episode:  530 score:  [1]  mean score 0.55 epsilon_val 0.00\n",
            "2\n",
            "episode:  540 score:  0\n",
            "100 averages -- episode:  540 score:  [0]  mean score 0.51 epsilon_val 0.00\n",
            "2\n",
            "episode:  550 score:  0\n",
            "100 averages -- episode:  550 score:  [0]  mean score 0.52 epsilon_val 0.00\n",
            "0\n",
            "episode:  560 score:  0\n",
            "100 averages -- episode:  560 score:  [1]  mean score 0.56 epsilon_val 0.00\n",
            "2\n",
            "episode:  570 score:  0\n",
            "100 averages -- episode:  570 score:  [0]  mean score 0.55 epsilon_val 0.00\n",
            "2\n",
            "episode:  580 score:  0\n",
            "100 averages -- episode:  580 score:  [1]  mean score 0.54 epsilon_val 0.00\n",
            "0\n",
            "episode:  590 score:  0\n",
            "100 averages -- episode:  590 score:  [0]  mean score 0.51 epsilon_val 0.00\n",
            "2\n",
            "episode:  600 score:  0\n",
            "100 averages -- episode:  600 score:  [0]  mean score 0.50 epsilon_val 0.00\n",
            "2\n",
            "episode:  610 score:  0\n",
            "100 averages -- episode:  610 score:  [1]  mean score 0.49 epsilon_val 0.00\n",
            "0\n",
            "episode:  620 score:  0\n",
            "100 averages -- episode:  620 score:  [0]  mean score 0.50 epsilon_val 0.00\n",
            "2\n",
            "episode:  630 score:  0\n",
            "100 averages -- episode:  630 score:  [1]  mean score 0.48 epsilon_val 0.00\n",
            "2\n",
            "episode:  640 score:  0\n",
            "100 averages -- episode:  640 score:  [0]  mean score 0.47 epsilon_val 0.00\n",
            "2\n",
            "episode:  650 score:  0\n",
            "100 averages -- episode:  650 score:  [1]  mean score 0.49 epsilon_val 0.00\n",
            "2\n",
            "episode:  660 score:  0\n",
            "100 averages -- episode:  660 score:  [0]  mean score 0.47 epsilon_val 0.00\n",
            "2\n",
            "episode:  670 score:  0\n",
            "100 averages -- episode:  670 score:  [1]  mean score 0.51 epsilon_val 0.00\n",
            "2\n",
            "episode:  680 score:  0\n",
            "100 averages -- episode:  680 score:  [1]  mean score 0.52 epsilon_val 0.00\n",
            "2\n",
            "episode:  690 score:  0\n",
            "100 averages -- episode:  690 score:  [0]  mean score 0.56 epsilon_val 0.00\n",
            "2\n",
            "episode:  700 score:  0\n",
            "100 averages -- episode:  700 score:  [1]  mean score 0.54 epsilon_val 0.00\n",
            "0\n",
            "episode:  710 score:  0\n",
            "100 averages -- episode:  710 score:  [0]  mean score 0.55 epsilon_val 0.00\n",
            "2\n",
            "episode:  720 score:  0\n",
            "100 averages -- episode:  720 score:  [0]  mean score 0.53 epsilon_val 0.00\n",
            "0\n",
            "episode:  730 score:  0\n",
            "100 averages -- episode:  730 score:  [1]  mean score 0.53 epsilon_val 0.00\n",
            "0\n",
            "episode:  740 score:  0\n",
            "100 averages -- episode:  740 score:  [1]  mean score 0.56 epsilon_val 0.00\n",
            "2\n",
            "episode:  750 score:  0\n",
            "100 averages -- episode:  750 score:  [0]  mean score 0.52 epsilon_val 0.00\n",
            "2\n",
            "episode:  760 score:  0\n",
            "100 averages -- episode:  760 score:  [1]  mean score 0.54 epsilon_val 0.00\n",
            "Of 1 Games in 18-19 EPL, we predicted 1.0\n",
            "2\n",
            "episode:  10 score:  0\n",
            "100 averages -- episode:  10 score:  [1]  mean score 0.30 epsilon_val 0.00\n",
            "2\n",
            "episode:  20 score:  0\n",
            "100 averages -- episode:  20 score:  [0]  mean score 0.35 epsilon_val 0.00\n",
            "2\n",
            "episode:  30 score:  0\n",
            "100 averages -- episode:  30 score:  [1]  mean score 0.40 epsilon_val 0.00\n",
            "2\n",
            "episode:  40 score:  0\n",
            "100 averages -- episode:  40 score:  [0]  mean score 0.38 epsilon_val 0.00\n",
            "0\n",
            "episode:  50 score:  0\n",
            "100 averages -- episode:  50 score:  [1]  mean score 0.46 epsilon_val 0.00\n",
            "2\n",
            "episode:  60 score:  0\n",
            "100 averages -- episode:  60 score:  [0]  mean score 0.47 epsilon_val 0.00\n",
            "0\n",
            "episode:  70 score:  0\n",
            "100 averages -- episode:  70 score:  [0]  mean score 0.43 epsilon_val 0.00\n",
            "2\n",
            "episode:  80 score:  0\n",
            "100 averages -- episode:  80 score:  [1]  mean score 0.45 epsilon_val 0.00\n",
            "2\n",
            "episode:  90 score:  0\n",
            "100 averages -- episode:  90 score:  [1]  mean score 0.43 epsilon_val 0.00\n",
            "2\n",
            "episode:  100 score:  0\n",
            "100 averages -- episode:  100 score:  [1]  mean score 0.47 epsilon_val 0.00\n",
            "0\n",
            "episode:  110 score:  0\n",
            "100 averages -- episode:  110 score:  [1]  mean score 0.48 epsilon_val 0.00\n",
            "2\n",
            "episode:  120 score:  0\n",
            "100 averages -- episode:  120 score:  [1]  mean score 0.50 epsilon_val 0.00\n",
            "2\n",
            "episode:  130 score:  0\n",
            "100 averages -- episode:  130 score:  [1]  mean score 0.50 epsilon_val 0.00\n",
            "2\n",
            "episode:  140 score:  0\n",
            "100 averages -- episode:  140 score:  [0]  mean score 0.53 epsilon_val 0.00\n",
            "2\n",
            "episode:  150 score:  0\n",
            "100 averages -- episode:  150 score:  [0]  mean score 0.51 epsilon_val 0.00\n",
            "2\n",
            "episode:  160 score:  0\n",
            "100 averages -- episode:  160 score:  [1]  mean score 0.54 epsilon_val 0.00\n",
            "0\n",
            "episode:  170 score:  0\n",
            "100 averages -- episode:  170 score:  [1]  mean score 0.58 epsilon_val 0.00\n",
            "0\n",
            "episode:  180 score:  0\n",
            "100 averages -- episode:  180 score:  [1]  mean score 0.59 epsilon_val 0.00\n",
            "0\n",
            "episode:  190 score:  0\n",
            "100 averages -- episode:  190 score:  [1]  mean score 0.63 epsilon_val 0.00\n",
            "2\n",
            "episode:  200 score:  0\n",
            "100 averages -- episode:  200 score:  [1]  mean score 0.61 epsilon_val 0.00\n",
            "0\n",
            "episode:  210 score:  0\n",
            "100 averages -- episode:  210 score:  [0]  mean score 0.62 epsilon_val 0.00\n",
            "2\n",
            "episode:  220 score:  0\n",
            "100 averages -- episode:  220 score:  [1]  mean score 0.62 epsilon_val 0.00\n",
            "0\n",
            "episode:  230 score:  0\n",
            "100 averages -- episode:  230 score:  [0]  mean score 0.60 epsilon_val 0.00\n",
            "2\n",
            "episode:  240 score:  0\n",
            "100 averages -- episode:  240 score:  [1]  mean score 0.62 epsilon_val 0.00\n",
            "0\n",
            "episode:  250 score:  0\n",
            "100 averages -- episode:  250 score:  [1]  mean score 0.61 epsilon_val 0.00\n",
            "2\n",
            "episode:  260 score:  0\n",
            "100 averages -- episode:  260 score:  [1]  mean score 0.59 epsilon_val 0.00\n",
            "0\n",
            "episode:  270 score:  0\n",
            "100 averages -- episode:  270 score:  [0]  mean score 0.58 epsilon_val 0.00\n",
            "2\n",
            "episode:  280 score:  0\n",
            "100 averages -- episode:  280 score:  [0]  mean score 0.58 epsilon_val 0.00\n",
            "2\n",
            "episode:  290 score:  0\n",
            "100 averages -- episode:  290 score:  [0]  mean score 0.56 epsilon_val 0.00\n",
            "2\n",
            "episode:  300 score:  0\n",
            "100 averages -- episode:  300 score:  [1]  mean score 0.57 epsilon_val 0.00\n",
            "2\n",
            "episode:  310 score:  0\n",
            "100 averages -- episode:  310 score:  [1]  mean score 0.58 epsilon_val 0.00\n",
            "2\n",
            "episode:  320 score:  0\n",
            "100 averages -- episode:  320 score:  [1]  mean score 0.59 epsilon_val 0.00\n",
            "2\n",
            "episode:  330 score:  0\n",
            "100 averages -- episode:  330 score:  [0]  mean score 0.63 epsilon_val 0.00\n",
            "2\n",
            "episode:  340 score:  0\n",
            "100 averages -- episode:  340 score:  [0]  mean score 0.58 epsilon_val 0.00\n",
            "2\n",
            "episode:  350 score:  0\n",
            "100 averages -- episode:  350 score:  [1]  mean score 0.57 epsilon_val 0.00\n",
            "0\n",
            "episode:  360 score:  0\n",
            "100 averages -- episode:  360 score:  [1]  mean score 0.59 epsilon_val 0.00\n",
            "0\n",
            "episode:  370 score:  0\n",
            "100 averages -- episode:  370 score:  [0]  mean score 0.62 epsilon_val 0.00\n",
            "0\n",
            "episode:  380 score:  0\n",
            "100 averages -- episode:  380 score:  [1]  mean score 0.62 epsilon_val 0.00\n",
            "0\n",
            "episode:  390 score:  0\n",
            "100 averages -- episode:  390 score:  [1]  mean score 0.63 epsilon_val 0.00\n",
            "2\n",
            "episode:  400 score:  0\n",
            "100 averages -- episode:  400 score:  [0]  mean score 0.61 epsilon_val 0.00\n",
            "2\n",
            "episode:  410 score:  0\n",
            "100 averages -- episode:  410 score:  [0]  mean score 0.59 epsilon_val 0.00\n",
            "2\n",
            "episode:  420 score:  0\n",
            "100 averages -- episode:  420 score:  [0]  mean score 0.57 epsilon_val 0.00\n",
            "2\n",
            "episode:  430 score:  0\n",
            "100 averages -- episode:  430 score:  [0]  mean score 0.55 epsilon_val 0.00\n",
            "2\n",
            "episode:  440 score:  0\n",
            "100 averages -- episode:  440 score:  [1]  mean score 0.60 epsilon_val 0.00\n",
            "0\n",
            "episode:  450 score:  0\n",
            "100 averages -- episode:  450 score:  [0]  mean score 0.61 epsilon_val 0.00\n",
            "2\n",
            "episode:  460 score:  0\n",
            "100 averages -- episode:  460 score:  [0]  mean score 0.55 epsilon_val 0.00\n",
            "2\n",
            "episode:  470 score:  0\n",
            "100 averages -- episode:  470 score:  [0]  mean score 0.52 epsilon_val 0.00\n",
            "2\n",
            "episode:  480 score:  0\n",
            "100 averages -- episode:  480 score:  [1]  mean score 0.51 epsilon_val 0.00\n",
            "2\n",
            "episode:  490 score:  0\n",
            "100 averages -- episode:  490 score:  [1]  mean score 0.51 epsilon_val 0.00\n",
            "2\n",
            "episode:  500 score:  0\n",
            "100 averages -- episode:  500 score:  [1]  mean score 0.53 epsilon_val 0.00\n",
            "2\n",
            "episode:  510 score:  0\n",
            "100 averages -- episode:  510 score:  [1]  mean score 0.54 epsilon_val 0.00\n",
            "0\n",
            "episode:  520 score:  0\n",
            "100 averages -- episode:  520 score:  [1]  mean score 0.54 epsilon_val 0.00\n",
            "2\n",
            "episode:  530 score:  0\n",
            "100 averages -- episode:  530 score:  [0]  mean score 0.55 epsilon_val 0.00\n",
            "0\n",
            "episode:  540 score:  0\n",
            "100 averages -- episode:  540 score:  [1]  mean score 0.51 epsilon_val 0.00\n",
            "0\n",
            "episode:  550 score:  0\n",
            "100 averages -- episode:  550 score:  [1]  mean score 0.52 epsilon_val 0.00\n",
            "2\n",
            "episode:  560 score:  0\n",
            "100 averages -- episode:  560 score:  [1]  mean score 0.56 epsilon_val 0.00\n",
            "2\n",
            "episode:  570 score:  0\n",
            "100 averages -- episode:  570 score:  [0]  mean score 0.55 epsilon_val 0.00\n",
            "0\n",
            "episode:  580 score:  0\n",
            "100 averages -- episode:  580 score:  [1]  mean score 0.54 epsilon_val 0.00\n",
            "2\n",
            "episode:  590 score:  0\n",
            "100 averages -- episode:  590 score:  [0]  mean score 0.51 epsilon_val 0.00\n",
            "2\n",
            "episode:  600 score:  0\n",
            "100 averages -- episode:  600 score:  [0]  mean score 0.50 epsilon_val 0.00\n",
            "2\n",
            "episode:  610 score:  0\n",
            "100 averages -- episode:  610 score:  [0]  mean score 0.49 epsilon_val 0.00\n",
            "0\n",
            "episode:  620 score:  0\n",
            "100 averages -- episode:  620 score:  [0]  mean score 0.50 epsilon_val 0.00\n",
            "2\n",
            "episode:  630 score:  0\n",
            "100 averages -- episode:  630 score:  [0]  mean score 0.48 epsilon_val 0.00\n",
            "0\n",
            "episode:  640 score:  0\n",
            "100 averages -- episode:  640 score:  [0]  mean score 0.47 epsilon_val 0.00\n",
            "2\n",
            "episode:  650 score:  0\n",
            "100 averages -- episode:  650 score:  [1]  mean score 0.49 epsilon_val 0.00\n",
            "2\n",
            "episode:  660 score:  0\n",
            "100 averages -- episode:  660 score:  [0]  mean score 0.47 epsilon_val 0.00\n",
            "0\n",
            "episode:  670 score:  0\n",
            "100 averages -- episode:  670 score:  [1]  mean score 0.51 epsilon_val 0.00\n",
            "0\n",
            "episode:  680 score:  0\n",
            "100 averages -- episode:  680 score:  [1]  mean score 0.52 epsilon_val 0.00\n",
            "2\n",
            "episode:  690 score:  0\n",
            "100 averages -- episode:  690 score:  [1]  mean score 0.56 epsilon_val 0.00\n",
            "0\n",
            "episode:  700 score:  0\n",
            "100 averages -- episode:  700 score:  [0]  mean score 0.54 epsilon_val 0.00\n",
            "2\n",
            "episode:  710 score:  0\n",
            "100 averages -- episode:  710 score:  [1]  mean score 0.55 epsilon_val 0.00\n",
            "0\n",
            "episode:  720 score:  0\n",
            "100 averages -- episode:  720 score:  [0]  mean score 0.53 epsilon_val 0.00\n",
            "2\n",
            "episode:  730 score:  0\n",
            "100 averages -- episode:  730 score:  [0]  mean score 0.53 epsilon_val 0.00\n",
            "2\n",
            "episode:  740 score:  0\n",
            "100 averages -- episode:  740 score:  [0]  mean score 0.56 epsilon_val 0.00\n",
            "0\n",
            "episode:  750 score:  0\n",
            "100 averages -- episode:  750 score:  [1]  mean score 0.52 epsilon_val 0.00\n",
            "2\n",
            "episode:  760 score:  0\n",
            "100 averages -- episode:  760 score:  [1]  mean score 0.54 epsilon_val 0.00\n",
            "Of 2 Games in 18-19 EPL, we predicted 1.0\n",
            "2\n",
            "episode:  770 score:  0\n",
            "100 averages -- episode:  770 score:  [1]  mean score 0.49 epsilon_val 0.00\n",
            "Of 12 Games in 18-19 EPL, we predicted 0.8333333333333334\n",
            "0\n",
            "episode:  780 score:  0\n",
            "100 averages -- episode:  780 score:  [1]  mean score 0.48 epsilon_val 0.00\n",
            "Of 22 Games in 18-19 EPL, we predicted 0.7727272727272727\n",
            "0\n",
            "episode:  790 score:  0\n",
            "100 averages -- episode:  790 score:  [0]  mean score 0.47 epsilon_val 0.00\n",
            "Of 32 Games in 18-19 EPL, we predicted 0.71875\n",
            "0\n",
            "episode:  800 score:  0\n",
            "100 averages -- episode:  800 score:  [1]  mean score 0.48 epsilon_val 0.00\n",
            "Of 42 Games in 18-19 EPL, we predicted 0.6666666666666666\n",
            "2\n",
            "episode:  810 score:  0\n",
            "100 averages -- episode:  810 score:  [1]  mean score 0.51 epsilon_val 0.00\n",
            "Of 52 Games in 18-19 EPL, we predicted 0.6346153846153846\n",
            "0\n",
            "episode:  820 score:  0\n",
            "100 averages -- episode:  820 score:  [1]  mean score 0.53 epsilon_val 0.00\n",
            "Of 62 Games in 18-19 EPL, we predicted 0.6290322580645161\n",
            "0\n",
            "episode:  830 score:  0\n",
            "100 averages -- episode:  830 score:  [0]  mean score 0.53 epsilon_val 0.00\n",
            "Of 72 Games in 18-19 EPL, we predicted 0.625\n",
            "0\n",
            "episode:  840 score:  0\n",
            "100 averages -- episode:  840 score:  [0]  mean score 0.52 epsilon_val 0.00\n",
            "Of 82 Games in 18-19 EPL, we predicted 0.6097560975609756\n",
            "2\n",
            "episode:  850 score:  0\n",
            "100 averages -- episode:  850 score:  [1]  mean score 0.52 epsilon_val 0.00\n",
            "Of 92 Games in 18-19 EPL, we predicted 0.6086956521739131\n",
            "0\n",
            "episode:  860 score:  0\n",
            "100 averages -- episode:  860 score:  [1]  mean score 0.53 epsilon_val 0.00\n",
            "Of 102 Games in 18-19 EPL, we predicted 0.6176470588235294\n",
            "2\n",
            "episode:  870 score:  0\n",
            "100 averages -- episode:  870 score:  [1]  mean score 0.54 epsilon_val 0.00\n",
            "Of 112 Games in 18-19 EPL, we predicted 0.6428571428571429\n",
            "2\n",
            "episode:  880 score:  0\n",
            "100 averages -- episode:  880 score:  [1]  mean score 0.55 epsilon_val 0.00\n",
            "Of 122 Games in 18-19 EPL, we predicted 0.6229508196721312\n",
            "2\n",
            "episode:  890 score:  0\n",
            "100 averages -- episode:  890 score:  [0]  mean score 0.55 epsilon_val 0.00\n",
            "Of 132 Games in 18-19 EPL, we predicted 0.6136363636363636\n",
            "2\n",
            "episode:  900 score:  0\n",
            "100 averages -- episode:  900 score:  [1]  mean score 0.56 epsilon_val 0.00\n",
            "Of 142 Games in 18-19 EPL, we predicted 0.6197183098591549\n",
            "0\n",
            "episode:  910 score:  0\n",
            "100 averages -- episode:  910 score:  [1]  mean score 0.54 epsilon_val 0.00\n",
            "Of 152 Games in 18-19 EPL, we predicted 0.618421052631579\n",
            "2\n",
            "episode:  920 score:  0\n",
            "100 averages -- episode:  920 score:  [0]  mean score 0.56 epsilon_val 0.00\n",
            "Of 162 Games in 18-19 EPL, we predicted 0.6234567901234568\n",
            "0\n",
            "episode:  930 score:  0\n",
            "100 averages -- episode:  930 score:  [1]  mean score 0.59 epsilon_val 0.00\n",
            "Of 172 Games in 18-19 EPL, we predicted 0.622093023255814\n",
            "0\n",
            "episode:  940 score:  0\n",
            "100 averages -- episode:  940 score:  [0]  mean score 0.61 epsilon_val 0.00\n",
            "Of 182 Games in 18-19 EPL, we predicted 0.6153846153846154\n",
            "0\n",
            "episode:  950 score:  0\n",
            "100 averages -- episode:  950 score:  [1]  mean score 0.63 epsilon_val 0.00\n",
            "Of 192 Games in 18-19 EPL, we predicted 0.609375\n",
            "0\n",
            "episode:  960 score:  0\n",
            "100 averages -- episode:  960 score:  [1]  mean score 0.62 epsilon_val 0.00\n",
            "Of 202 Games in 18-19 EPL, we predicted 0.6089108910891089\n",
            "2\n",
            "episode:  970 score:  0\n",
            "100 averages -- episode:  970 score:  [0]  mean score 0.65 epsilon_val 0.00\n",
            "Of 212 Games in 18-19 EPL, we predicted 0.6037735849056604\n",
            "2\n",
            "episode:  980 score:  0\n",
            "100 averages -- episode:  980 score:  [0]  mean score 0.64 epsilon_val 0.00\n",
            "Of 222 Games in 18-19 EPL, we predicted 0.5990990990990991\n",
            "0\n",
            "episode:  990 score:  0\n",
            "100 averages -- episode:  990 score:  [1]  mean score 0.61 epsilon_val 0.00\n",
            "Of 232 Games in 18-19 EPL, we predicted 0.603448275862069\n",
            "2\n",
            "episode:  1000 score:  0\n",
            "100 averages -- episode:  1000 score:  [0]  mean score 0.63 epsilon_val 0.00\n",
            "Of 242 Games in 18-19 EPL, we predicted 0.5950413223140496\n",
            "0\n",
            "episode:  1010 score:  0\n",
            "100 averages -- episode:  1010 score:  [1]  mean score 0.62 epsilon_val 0.00\n",
            "Of 252 Games in 18-19 EPL, we predicted 0.5952380952380952\n",
            "2\n",
            "episode:  1020 score:  0\n",
            "100 averages -- episode:  1020 score:  [1]  mean score 0.61 epsilon_val 0.00\n",
            "Of 262 Games in 18-19 EPL, we predicted 0.5992366412213741\n",
            "2\n",
            "episode:  1030 score:  0\n",
            "100 averages -- episode:  1030 score:  [0]  mean score 0.60 epsilon_val 0.00\n",
            "Of 272 Games in 18-19 EPL, we predicted 0.5919117647058824\n",
            "0\n",
            "episode:  1040 score:  0\n",
            "100 averages -- episode:  1040 score:  [1]  mean score 0.60 epsilon_val 0.00\n",
            "Of 282 Games in 18-19 EPL, we predicted 0.599290780141844\n",
            "2\n",
            "episode:  1050 score:  0\n",
            "100 averages -- episode:  1050 score:  [0]  mean score 0.59 epsilon_val 0.00\n",
            "Of 292 Games in 18-19 EPL, we predicted 0.6027397260273972\n",
            "2\n",
            "episode:  1060 score:  0\n",
            "100 averages -- episode:  1060 score:  [1]  mean score 0.59 epsilon_val 0.00\n",
            "Of 302 Games in 18-19 EPL, we predicted 0.5993377483443708\n",
            "2\n",
            "episode:  1070 score:  0\n",
            "100 averages -- episode:  1070 score:  [1]  mean score 0.59 epsilon_val 0.00\n",
            "Of 312 Games in 18-19 EPL, we predicted 0.6025641025641025\n",
            "0\n",
            "episode:  1080 score:  0\n",
            "100 averages -- episode:  1080 score:  [0]  mean score 0.61 epsilon_val 0.00\n",
            "Of 322 Games in 18-19 EPL, we predicted 0.6024844720496895\n",
            "2\n",
            "episode:  1090 score:  0\n",
            "100 averages -- episode:  1090 score:  [0]  mean score 0.65 epsilon_val 0.00\n",
            "Of 332 Games in 18-19 EPL, we predicted 0.5993975903614458\n",
            "2\n",
            "episode:  1100 score:  0\n",
            "100 averages -- episode:  1100 score:  [1]  mean score 0.61 epsilon_val 0.00\n",
            "Of 342 Games in 18-19 EPL, we predicted 0.5994152046783626\n",
            "0\n",
            "episode:  1110 score:  0\n",
            "100 averages -- episode:  1110 score:  [1]  mean score 0.59 epsilon_val 0.00\n",
            "Of 352 Games in 18-19 EPL, we predicted 0.5909090909090909\n",
            "2\n",
            "episode:  1120 score:  0\n",
            "100 averages -- episode:  1120 score:  [1]  mean score 0.60 epsilon_val 0.00\n",
            "Of 362 Games in 18-19 EPL, we predicted 0.585635359116022\n",
            "2\n",
            "episode:  1130 score:  0\n",
            "100 averages -- episode:  1130 score:  [0]  mean score 0.62 epsilon_val 0.00\n",
            "Of 372 Games in 18-19 EPL, we predicted 0.5887096774193549\n",
            "0.5800524934383202\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}